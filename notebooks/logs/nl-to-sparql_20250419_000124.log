2025-04-19 00:01:24,541 | [32mINFO[0m | nl-to-sparql | logging_utils.py:132 | Logging initialized: console=INFO, file=DEBUG
2025-04-19 00:01:26,345 | [32mINFO[0m | sentence_transformers.SentenceTransformer | SentenceTransformer.py:211 | Use pytorch device_name: mps
2025-04-19 00:01:26,345 | [32mINFO[0m | sentence_transformers.SentenceTransformer | SentenceTransformer.py:219 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-04-19 00:01:26,363 | DEBUG | urllib3.connectionpool | connectionpool.py:1049 | Starting new HTTPS connection (1): huggingface.co:443
2025-04-19 00:01:26,970 | DEBUG | urllib3.connectionpool | connectionpool.py:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 200 0
2025-04-19 00:01:27,273 | DEBUG | urllib3.connectionpool | connectionpool.py:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 200 0
2025-04-19 00:01:27,584 | DEBUG | urllib3.connectionpool | connectionpool.py:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 200 0
2025-04-19 00:01:27,886 | DEBUG | urllib3.connectionpool | connectionpool.py:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 200 0
2025-04-19 00:01:28,261 | DEBUG | urllib3.connectionpool | connectionpool.py:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1" 200 0
2025-04-19 00:01:28,587 | DEBUG | urllib3.connectionpool | connectionpool.py:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1" 404 0
2025-04-19 00:01:30,979 | DEBUG | urllib3.connectionpool | connectionpool.py:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 200 0
2025-04-19 00:01:32,725 | DEBUG | urllib3.connectionpool | connectionpool.py:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2025-04-19 00:01:33,094 | DEBUG | urllib3.connectionpool | connectionpool.py:544 | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/revision/main HTTP/1.1" 200 6761
2025-04-19 00:01:33,503 | DEBUG | urllib3.connectionpool | connectionpool.py:544 | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1" 200 6761
2025-04-19 00:01:34,298 | DEBUG | urllib3.connectionpool | connectionpool.py:544 | https://huggingface.co:443 "HEAD /cross-encoder/ms-marco-MiniLM-L-6-v2/resolve/main/config.json HTTP/1.1" 307 0
2025-04-19 00:01:34,712 | DEBUG | urllib3.connectionpool | connectionpool.py:544 | https://huggingface.co:443 "HEAD /cross-encoder/ms-marco-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 200 0
2025-04-19 00:01:35,122 | DEBUG | urllib3.connectionpool | connectionpool.py:544 | https://huggingface.co:443 "HEAD /cross-encoder/ms-marco-MiniLM-L-6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2025-04-19 00:01:35,435 | DEBUG | urllib3.connectionpool | connectionpool.py:544 | https://huggingface.co:443 "HEAD /cross-encoder/ms-marco-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2025-04-19 00:01:35,799 | DEBUG | urllib3.connectionpool | connectionpool.py:544 | https://huggingface.co:443 "HEAD /cross-encoder/ms-marco-MiniLM-L-6-v2/resolve/main/README.md HTTP/1.1" 307 0
2025-04-19 00:01:36,120 | DEBUG | urllib3.connectionpool | connectionpool.py:544 | https://huggingface.co:443 "HEAD /cross-encoder/ms-marco-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 200 0
2025-04-19 00:01:36,123 | [32mINFO[0m | sentence_transformers.cross_encoder.CrossEncoder | CrossEncoder.py:205 | Use pytorch device: mps
2025-04-19 00:01:36,978 | DEBUG | urllib3.connectionpool | connectionpool.py:544 | https://huggingface.co:443 "GET /api/models/cross-encoder/ms-marco-MiniLM-L-6-v2 HTTP/1.1" 307 82
2025-04-19 00:01:37,377 | DEBUG | urllib3.connectionpool | connectionpool.py:544 | https://huggingface.co:443 "GET /api/models/cross-encoder/ms-marco-MiniLM-L6-v2 HTTP/1.1" 200 5299
2025-04-19 00:01:37,784 | DEBUG | urllib3.connectionpool | connectionpool.py:544 | https://huggingface.co:443 "HEAD /urchade/gliner_medium-v2.1/resolve/main/config.json HTTP/1.1" 404 0
2025-04-19 00:01:38,118 | DEBUG | urllib3.connectionpool | connectionpool.py:544 | https://huggingface.co:443 "GET /api/models/urchade/gliner_medium-v2.1/revision/main HTTP/1.1" 200 1507
2025-04-19 00:01:38,442 | DEBUG | urllib3.connectionpool | connectionpool.py:544 | https://huggingface.co:443 "HEAD /microsoft/deberta-v3-base/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2025-04-19 00:01:39,516 | DEBUG | urllib3.connectionpool | connectionpool.py:544 | https://huggingface.co:443 "HEAD /microsoft/deberta-v3-base/resolve/main/config.json HTTP/1.1" 200 0
2025-04-19 00:02:24,096 | [32mINFO[0m | sentence_transformers.SentenceTransformer | SentenceTransformer.py:211 | Use pytorch device_name: mps
2025-04-19 00:02:24,100 | [32mINFO[0m | sentence_transformers.SentenceTransformer | SentenceTransformer.py:219 | Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-04-19 00:02:24,467 | DEBUG | urllib3.connectionpool | connectionpool.py:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 200 0
2025-04-19 00:02:24,787 | DEBUG | urllib3.connectionpool | connectionpool.py:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 200 0
2025-04-19 00:02:25,141 | DEBUG | urllib3.connectionpool | connectionpool.py:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 200 0
2025-04-19 00:02:25,502 | DEBUG | urllib3.connectionpool | connectionpool.py:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 200 0
2025-04-19 00:02:25,826 | DEBUG | urllib3.connectionpool | connectionpool.py:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1" 200 0
2025-04-19 00:02:31,098 | DEBUG | urllib3.connectionpool | connectionpool.py:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1" 404 0
2025-04-19 00:02:31,545 | DEBUG | urllib3.connectionpool | connectionpool.py:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 200 0
2025-04-19 00:02:32,016 | DEBUG | urllib3.connectionpool | connectionpool.py:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2025-04-19 00:02:32,377 | DEBUG | urllib3.connectionpool | connectionpool.py:544 | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/revision/main HTTP/1.1" 200 6761
2025-04-19 00:02:32,775 | DEBUG | urllib3.connectionpool | connectionpool.py:544 | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1" 200 6761
2025-04-19 00:02:33,941 | DEBUG | httpcore.connection | _trace.py:47 | connect_tcp.started host='localhost' port=6333 local_address=None timeout=5.0 socket_options=None
2025-04-19 00:02:33,943 | DEBUG | httpcore.connection | _trace.py:47 | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x174eb1af0>
2025-04-19 00:02:33,943 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_headers.started request=<Request [b'GET']>
2025-04-19 00:02:33,943 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_headers.complete
2025-04-19 00:02:33,944 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_body.started request=<Request [b'GET']>
2025-04-19 00:02:33,944 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_body.complete
2025-04-19 00:02:33,944 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_headers.started request=<Request [b'GET']>
2025-04-19 00:02:34,002 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'transfer-encoding', b'chunked'), (b'vary', b'accept-encoding, Origin, Access-Control-Request-Method, Access-Control-Request-Headers'), (b'content-encoding', b'zstd'), (b'content-type', b'application/json'), (b'date', b'Fri, 18 Apr 2025 17:02:33 GMT')])
2025-04-19 00:02:34,003 | [32mINFO[0m | httpx | _client.py:1025 | HTTP Request: GET http://localhost:6333 "HTTP/1.1 200 OK"
2025-04-19 00:02:34,008 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_body.started request=<Request [b'GET']>
2025-04-19 00:02:34,010 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_body.complete
2025-04-19 00:02:34,011 | DEBUG | httpcore.http11 | _trace.py:47 | response_closed.started
2025-04-19 00:02:34,011 | DEBUG | httpcore.http11 | _trace.py:47 | response_closed.complete
2025-04-19 00:02:34,011 | DEBUG | httpcore.connection | _trace.py:47 | close.started
2025-04-19 00:02:34,011 | DEBUG | httpcore.connection | _trace.py:47 | close.complete
2025-04-19 00:02:34,038 | DEBUG | docker.utils.config | config.py:21 | Trying paths: ['/Users/thanhhoang/.docker/config.json', '/Users/thanhhoang/.dockercfg']
2025-04-19 00:02:34,038 | DEBUG | docker.utils.config | config.py:25 | Found file at path: /Users/thanhhoang/.docker/config.json
2025-04-19 00:02:34,038 | DEBUG | docker.auth | auth.py:170 | Found 'auths' section
2025-04-19 00:02:34,038 | DEBUG | docker.auth | auth.py:123 | Auth data for https://index.docker.io/v1/ is absent. Client might be using a credentials store instead.
2025-04-19 00:02:34,038 | DEBUG | docker.auth | auth.py:177 | Found 'credsStore' section
2025-04-19 00:02:34,422 | DEBUG | urllib3.connectionpool | connectionpool.py:544 | http://localhost:None "GET /version HTTP/1.1" 200 None
2025-04-19 00:02:34,434 | DEBUG | urllib3.connectionpool | connectionpool.py:544 | http://localhost:None "GET /v1.43/_ping HTTP/1.1" 200 None
2025-04-19 00:02:34,628 | [32mINFO[0m | sentence_transformers.SentenceTransformer | SentenceTransformer.py:211 | Use pytorch device_name: mps
2025-04-19 00:02:34,629 | [32mINFO[0m | sentence_transformers.SentenceTransformer | SentenceTransformer.py:219 | Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-04-19 00:02:35,017 | DEBUG | urllib3.connectionpool | connectionpool.py:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 200 0
2025-04-19 00:02:35,324 | DEBUG | urllib3.connectionpool | connectionpool.py:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 200 0
2025-04-19 00:02:35,643 | DEBUG | urllib3.connectionpool | connectionpool.py:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 200 0
2025-04-19 00:02:36,034 | DEBUG | urllib3.connectionpool | connectionpool.py:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 200 0
2025-04-19 00:02:36,333 | DEBUG | urllib3.connectionpool | connectionpool.py:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1" 200 0
2025-04-19 00:02:39,367 | DEBUG | urllib3.connectionpool | connectionpool.py:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1" 404 0
2025-04-19 00:02:39,721 | DEBUG | urllib3.connectionpool | connectionpool.py:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 200 0
2025-04-19 00:02:40,111 | DEBUG | urllib3.connectionpool | connectionpool.py:544 | https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2025-04-19 00:02:40,554 | DEBUG | urllib3.connectionpool | connectionpool.py:544 | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/revision/main HTTP/1.1" 200 6761
2025-04-19 00:02:40,896 | DEBUG | urllib3.connectionpool | connectionpool.py:544 | https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1" 200 6761
2025-04-19 00:02:41,812 | DEBUG | httpcore.connection | _trace.py:47 | connect_tcp.started host='localhost' port=6333 local_address=None timeout=5.0 socket_options=None
2025-04-19 00:02:41,814 | DEBUG | httpcore.connection | _trace.py:47 | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x173544a30>
2025-04-19 00:02:41,815 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_headers.started request=<Request [b'GET']>
2025-04-19 00:02:41,817 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_headers.complete
2025-04-19 00:02:41,817 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_body.started request=<Request [b'GET']>
2025-04-19 00:02:41,818 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_body.complete
2025-04-19 00:02:41,818 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_headers.started request=<Request [b'GET']>
2025-04-19 00:02:41,944 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'transfer-encoding', b'chunked'), (b'content-encoding', b'zstd'), (b'vary', b'accept-encoding, Origin, Access-Control-Request-Method, Access-Control-Request-Headers'), (b'content-type', b'application/json'), (b'date', b'Fri, 18 Apr 2025 17:02:41 GMT')])
2025-04-19 00:02:41,952 | [32mINFO[0m | httpx | _client.py:1025 | HTTP Request: GET http://localhost:6333 "HTTP/1.1 200 OK"
2025-04-19 00:02:42,020 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_body.started request=<Request [b'GET']>
2025-04-19 00:02:42,023 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_body.complete
2025-04-19 00:02:42,024 | DEBUG | httpcore.http11 | _trace.py:47 | response_closed.started
2025-04-19 00:02:42,024 | DEBUG | httpcore.http11 | _trace.py:47 | response_closed.complete
2025-04-19 00:02:42,025 | DEBUG | httpcore.connection | _trace.py:47 | close.started
2025-04-19 00:02:42,029 | DEBUG | httpcore.connection | _trace.py:47 | close.complete
2025-04-19 00:03:11,551 | DEBUG | httpcore.connection | _trace.py:47 | connect_tcp.started host='localhost' port=6333 local_address=None timeout=5.0 socket_options=None
2025-04-19 00:03:11,552 | DEBUG | httpcore.connection | _trace.py:47 | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x30d3ee070>
2025-04-19 00:03:11,553 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_headers.started request=<Request [b'POST']>
2025-04-19 00:03:11,553 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_headers.complete
2025-04-19 00:03:11,553 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_body.started request=<Request [b'POST']>
2025-04-19 00:03:11,553 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_body.complete
2025-04-19 00:03:11,553 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_headers.started request=<Request [b'POST']>
2025-04-19 00:03:11,638 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'transfer-encoding', b'chunked'), (b'content-type', b'application/json'), (b'content-encoding', b'zstd'), (b'vary', b'accept-encoding, Origin, Access-Control-Request-Method, Access-Control-Request-Headers'), (b'date', b'Fri, 18 Apr 2025 17:03:11 GMT')])
2025-04-19 00:03:11,638 | [32mINFO[0m | httpx | _client.py:1025 | HTTP Request: POST http://localhost:6333/collections/refinement_examples/points/query "HTTP/1.1 200 OK"
2025-04-19 00:03:11,639 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_body.started request=<Request [b'POST']>
2025-04-19 00:03:11,639 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_body.complete
2025-04-19 00:03:11,640 | DEBUG | httpcore.http11 | _trace.py:47 | response_closed.started
2025-04-19 00:03:11,640 | DEBUG | httpcore.http11 | _trace.py:47 | response_closed.complete
2025-04-19 00:03:11,640 | DEBUG | httpcore.connection | _trace.py:47 | close.started
2025-04-19 00:03:11,640 | DEBUG | httpcore.connection | _trace.py:47 | close.complete
2025-04-19 00:03:11,688 | DEBUG | openai._base_client | _base_client.py:454 | Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'You specialize in understanding and refining user queries about knowledge graphs.\nTransform vague, ambiguous, or context-dependent questions into clear, standalone questions that are\nsuitable for conversion to SPARQL. Use conversation history to fill in missing details.\nYour output should be a refined query that captures the semantic intent clearly.', 'role': 'system'}, {'content': "I need you to refine the following user query about a knowledge graph or ontology into a clear, standalone question.\nUse the conversation history to fill in any missing context, resolve references, \nand make sure the query is complete and unambiguous.\n\nCurrent Conversation History:\n\n\nUser Query: What is the identify of Quang Trung\n\nSimilar Examples of Query Refinement:\n\n\nPlease provide the refined query as a standalone question. \nDo not include explanations or additional text, just the refined query.\nEnsure the refined query would make sense even without the conversation history.\nInclude specific entity names whenever they're referenced directly or indirectly.\n", 'role': 'user', 'name': 'QueryRefinementProxy'}], 'model': 'gpt-4o-mini', 'stream': False, 'temperature': 0.0}}
2025-04-19 00:03:11,689 | DEBUG | openai._base_client | _base_client.py:957 | Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-04-19 00:03:11,689 | DEBUG | httpcore.connection | _trace.py:47 | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=600 socket_options=None
2025-04-19 00:03:11,724 | DEBUG | httpcore.connection | _trace.py:47 | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1737eddc0>
2025-04-19 00:03:11,724 | DEBUG | httpcore.connection | _trace.py:47 | start_tls.started ssl_context=<ssl.SSLContext object at 0x30d5ffac0> server_hostname='api.openai.com' timeout=600
2025-04-19 00:03:11,759 | DEBUG | httpcore.connection | _trace.py:47 | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1737edb50>
2025-04-19 00:03:11,759 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_headers.started request=<Request [b'POST']>
2025-04-19 00:03:11,759 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_headers.complete
2025-04-19 00:03:11,759 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_body.started request=<Request [b'POST']>
2025-04-19 00:03:11,759 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_body.complete
2025-04-19 00:03:11,759 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_headers.started request=<Request [b'POST']>
2025-04-19 00:03:13,055 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 18 Apr 2025 17:03:13 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-azcwg4thexa6yyypiursgjjt'), (b'openai-processing-ms', b'518'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'4000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'3999742'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'3ms'), (b'x-request-id', b'req_83156069d6d8b1bd62505e6791c426b4'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=ThgjhCPnyp36E.huDtoqzUxCag6jtoEZ6ErxdxyeXP8-1744995793-1.0.1.1-eu1tinOR0RSxVNjDIo3JczPa2_xRZmOME1xQMnt8nuFFXs3a5KI8aKG9V2B6OXQdGyL_G4oyVs7_1HIXHMdvJDOX3PnqhFu93keuW_aCIX8; path=/; expires=Fri, 18-Apr-25 17:33:13 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=0ojPr11_bvzwEHE0LAGignmzK7mDlGrVNXalFk9KkU0-1744995793066-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9325bbf2b8f802be-HKG'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-04-19 00:03:13,055 | [32mINFO[0m | httpx | _client.py:1025 | HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-04-19 00:03:13,063 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_body.started request=<Request [b'POST']>
2025-04-19 00:03:13,063 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_body.complete
2025-04-19 00:03:13,063 | DEBUG | httpcore.http11 | _trace.py:47 | response_closed.started
2025-04-19 00:03:13,063 | DEBUG | httpcore.http11 | _trace.py:47 | response_closed.complete
2025-04-19 00:03:13,063 | DEBUG | openai._base_client | _base_client.py:996 | HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers([('date', 'Fri, 18 Apr 2025 17:03:13 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-organization', 'user-azcwg4thexa6yyypiursgjjt'), ('openai-processing-ms', '518'), ('openai-version', '2020-10-01'), ('x-ratelimit-limit-requests', '5000'), ('x-ratelimit-limit-tokens', '4000000'), ('x-ratelimit-remaining-requests', '4999'), ('x-ratelimit-remaining-tokens', '3999742'), ('x-ratelimit-reset-requests', '12ms'), ('x-ratelimit-reset-tokens', '3ms'), ('x-request-id', 'req_83156069d6d8b1bd62505e6791c426b4'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=ThgjhCPnyp36E.huDtoqzUxCag6jtoEZ6ErxdxyeXP8-1744995793-1.0.1.1-eu1tinOR0RSxVNjDIo3JczPa2_xRZmOME1xQMnt8nuFFXs3a5KI8aKG9V2B6OXQdGyL_G4oyVs7_1HIXHMdvJDOX3PnqhFu93keuW_aCIX8; path=/; expires=Fri, 18-Apr-25 17:33:13 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=0ojPr11_bvzwEHE0LAGignmzK7mDlGrVNXalFk9KkU0-1744995793066-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '9325bbf2b8f802be-HKG'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=":443"; ma=86400')])
2025-04-19 00:03:13,063 | DEBUG | openai._base_client | _base_client.py:1004 | request_id: req_83156069d6d8b1bd62505e6791c426b4
2025-04-19 00:03:14,726 | DEBUG | openai._base_client | _base_client.py:454 | Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'You are a professional developer with experience in writing SPARQL for ontology file. Your task is to create a plan to transform the provided natural query to SPARQL. Please follow the detailed instruction below:\n        - If a query need to compute or find out the interval of time, divide query into simple natural queries and merge queries in last step. Else please not change the query.\n        - Add level for each step: **simple**: can generate query immediately and **complex**: must use previous queries.\n        - Add type of SPARQL query for each step: SELECT, ASK, DESCRIBE and CONSTRUCT.\n        - Do not create SPARQL query.\n        - If can not create plan to transform to SPARQL, the output is []\n        The output format must be following this format: \n        [{"step": "step query", "sparql_type": "SELECT or ASK or DESCRIBE or CONSTRUCT", "level": "simple or complex"}, ...]', 'role': 'system'}, {'content': 'What is the identity of Quang Trung in the knowledge graph?', 'role': 'user'}], 'model': 'gpt-4o-mini', 'stream': False, 'temperature': 0.0}}
2025-04-19 00:03:14,727 | DEBUG | openai._base_client | _base_client.py:957 | Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-04-19 00:03:14,727 | DEBUG | httpcore.connection | _trace.py:47 | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None
2025-04-19 00:03:14,756 | DEBUG | httpcore.connection | _trace.py:47 | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1737ed640>
2025-04-19 00:03:14,756 | DEBUG | httpcore.connection | _trace.py:47 | start_tls.started ssl_context=<ssl.SSLContext object at 0x30d5ffc40> server_hostname='api.openai.com' timeout=None
2025-04-19 00:03:14,795 | DEBUG | httpcore.connection | _trace.py:47 | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1737ed8b0>
2025-04-19 00:03:14,796 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_headers.started request=<Request [b'POST']>
2025-04-19 00:03:14,796 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_headers.complete
2025-04-19 00:03:14,796 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_body.started request=<Request [b'POST']>
2025-04-19 00:03:14,796 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_body.complete
2025-04-19 00:03:14,796 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_headers.started request=<Request [b'POST']>
2025-04-19 00:03:16,385 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 18 Apr 2025 17:03:16 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-azcwg4thexa6yyypiursgjjt'), (b'openai-processing-ms', b'870'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'4000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'3999758'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'3ms'), (b'x-request-id', b'req_59ff2bc062987dbb2870ed92aafd1ac9'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=KPPZ2589D3YeKNYmZBR8VdDWT.LcIARapT5RTXW6fNc-1744995796-1.0.1.1-BeP6Zkb869JEenbz_krIGxU1ZPW4Ki8Ll8niyVUbpjonNuZRSSe7cEury2.UQFBRSWiyM9Lz0ZSvEwO_akc6FeB9cxH46M6Sja2h2AqdH.0; path=/; expires=Fri, 18-Apr-25 17:33:16 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=lIcksy7tS.fkduF46zmuK9ir_rfk3R0Z_s2OU4eOk80-1744995796393-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9325bc05b93304ec-HKG'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-04-19 00:03:16,386 | [32mINFO[0m | httpx | _client.py:1025 | HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-04-19 00:03:16,387 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_body.started request=<Request [b'POST']>
2025-04-19 00:03:16,389 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_body.complete
2025-04-19 00:03:16,390 | DEBUG | httpcore.http11 | _trace.py:47 | response_closed.started
2025-04-19 00:03:16,390 | DEBUG | httpcore.http11 | _trace.py:47 | response_closed.complete
2025-04-19 00:03:16,390 | DEBUG | openai._base_client | _base_client.py:996 | HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers([('date', 'Fri, 18 Apr 2025 17:03:16 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-organization', 'user-azcwg4thexa6yyypiursgjjt'), ('openai-processing-ms', '870'), ('openai-version', '2020-10-01'), ('x-ratelimit-limit-requests', '5000'), ('x-ratelimit-limit-tokens', '4000000'), ('x-ratelimit-remaining-requests', '4999'), ('x-ratelimit-remaining-tokens', '3999758'), ('x-ratelimit-reset-requests', '12ms'), ('x-ratelimit-reset-tokens', '3ms'), ('x-request-id', 'req_59ff2bc062987dbb2870ed92aafd1ac9'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=KPPZ2589D3YeKNYmZBR8VdDWT.LcIARapT5RTXW6fNc-1744995796-1.0.1.1-BeP6Zkb869JEenbz_krIGxU1ZPW4Ki8Ll8niyVUbpjonNuZRSSe7cEury2.UQFBRSWiyM9Lz0ZSvEwO_akc6FeB9cxH46M6Sja2h2AqdH.0; path=/; expires=Fri, 18-Apr-25 17:33:16 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=lIcksy7tS.fkduF46zmuK9ir_rfk3R0Z_s2OU4eOk80-1744995796393-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '9325bc05b93304ec-HKG'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=":443"; ma=86400')])
2025-04-19 00:03:16,390 | DEBUG | openai._base_client | _base_client.py:1004 | request_id: req_59ff2bc062987dbb2870ed92aafd1ac9
2025-04-19 00:03:16,399 | DEBUG | openai._base_client | _base_client.py:454 | Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'You are a professional developer with experience in writing SPARQL for ontology file. Your task is to validate the provided plan to transform natural provided user query to SPARQL is valid. Please follow the instruction below:\n             - Check each step is correct. If not correct, provide how to improve in short for each step\n             - If the step level is complex, if all property is describe in previous steps, it is correct. \n             - Check if the plan can answer the user query.\n             - If plan is valid or can be accepted, the output is {"is_valid": true, "feedback": []}\n             The output format must be following this format:\n             {"is_valid": true or false, "feedback": [\n                {"step": "step query", "feedback": "feedback"}\n             ]}', 'role': 'system'}, {'content': "**User query:**What is the identity of Quang Trung in the knowledge graph?, **Plan**:[{'step': 'Identify the entity Quang Trung in the knowledge graph', 'sparql_type': 'SELECT', 'level': 'simple'}]", 'role': 'user'}], 'model': 'gpt-4o-mini', 'stream': False, 'temperature': 0.0}}
2025-04-19 00:03:16,399 | DEBUG | openai._base_client | _base_client.py:957 | Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-04-19 00:03:16,399 | DEBUG | httpcore.connection | _trace.py:47 | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None
2025-04-19 00:03:16,433 | DEBUG | httpcore.connection | _trace.py:47 | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x17504dbe0>
2025-04-19 00:03:16,433 | DEBUG | httpcore.connection | _trace.py:47 | start_tls.started ssl_context=<ssl.SSLContext object at 0x30d5ffd40> server_hostname='api.openai.com' timeout=None
2025-04-19 00:03:16,471 | DEBUG | httpcore.connection | _trace.py:47 | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1737ed4f0>
2025-04-19 00:03:16,471 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_headers.started request=<Request [b'POST']>
2025-04-19 00:03:16,471 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_headers.complete
2025-04-19 00:03:16,471 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_body.started request=<Request [b'POST']>
2025-04-19 00:03:16,471 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_body.complete
2025-04-19 00:03:16,471 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_headers.started request=<Request [b'POST']>
2025-04-19 00:03:19,344 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 18 Apr 2025 17:03:19 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-azcwg4thexa6yyypiursgjjt'), (b'openai-processing-ms', b'2076'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'4000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'3999747'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'3ms'), (b'x-request-id', b'req_fe47a91a590e99e501e1a19eb86d1bfe'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=PVsC3L4BcQmwDJMs878R0pvOdCD1vztqnOdnKo2VQUg-1744995799-1.0.1.1-En_xNqglyvDkUSBNYYuuCZY7xb4n6xj7ndKyG4138Wb4JNrWll0i5uTaf6SWij6HnHyQPOmSSKYjYPtKWfUuFvqcgXz2s_jAOMGKI0SKeu8; path=/; expires=Fri, 18-Apr-25 17:33:19 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=fmfA7SCJblXRNJqBZsLAfgmzktArS5Apm4axqsK89_Y-1744995799351-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9325bc103be10ed4-HKG'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-04-19 00:03:19,345 | [32mINFO[0m | httpx | _client.py:1025 | HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-04-19 00:03:19,345 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_body.started request=<Request [b'POST']>
2025-04-19 00:03:19,348 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_body.complete
2025-04-19 00:03:19,348 | DEBUG | httpcore.http11 | _trace.py:47 | response_closed.started
2025-04-19 00:03:19,348 | DEBUG | httpcore.http11 | _trace.py:47 | response_closed.complete
2025-04-19 00:03:19,348 | DEBUG | openai._base_client | _base_client.py:996 | HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers([('date', 'Fri, 18 Apr 2025 17:03:19 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-organization', 'user-azcwg4thexa6yyypiursgjjt'), ('openai-processing-ms', '2076'), ('openai-version', '2020-10-01'), ('x-ratelimit-limit-requests', '5000'), ('x-ratelimit-limit-tokens', '4000000'), ('x-ratelimit-remaining-requests', '4999'), ('x-ratelimit-remaining-tokens', '3999747'), ('x-ratelimit-reset-requests', '12ms'), ('x-ratelimit-reset-tokens', '3ms'), ('x-request-id', 'req_fe47a91a590e99e501e1a19eb86d1bfe'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=PVsC3L4BcQmwDJMs878R0pvOdCD1vztqnOdnKo2VQUg-1744995799-1.0.1.1-En_xNqglyvDkUSBNYYuuCZY7xb4n6xj7ndKyG4138Wb4JNrWll0i5uTaf6SWij6HnHyQPOmSSKYjYPtKWfUuFvqcgXz2s_jAOMGKI0SKeu8; path=/; expires=Fri, 18-Apr-25 17:33:19 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=fmfA7SCJblXRNJqBZsLAfgmzktArS5Apm4axqsK89_Y-1744995799351-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '9325bc103be10ed4-HKG'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=":443"; ma=86400')])
2025-04-19 00:03:19,348 | DEBUG | openai._base_client | _base_client.py:1004 | request_id: req_fe47a91a590e99e501e1a19eb86d1bfe
2025-04-19 00:03:19,349 | DEBUG | openai._base_client | _base_client.py:454 | Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'You are a professional developer with experience in writing SPARQL for ontology file. Your task is to create a plan to transform the provided natural query to SPARQL. Please follow the detailed instruction below:\n        - If a query need to compute or find out the interval of time, divide query into simple natural queries and merge queries in last step. Else please not change the query.\n        - Add level for each step: **simple**: can generate query immediately and **complex**: must use previous queries.\n        - Add type of SPARQL query for each step: SELECT, ASK, DESCRIBE and CONSTRUCT.\n        - Do not create SPARQL query.\n        - If can not create plan to transform to SPARQL, the output is []\n        The output format must be following this format: \n        [{"step": "step query", "sparql_type": "SELECT or ASK or DESCRIBE or CONSTRUCT", "level": "simple or complex"}, ...]', 'role': 'system'}, {'content': 'What is the identity of Quang Trung in the knowledge graph?', 'role': 'user'}], 'model': 'gpt-4o-mini', 'stream': False, 'temperature': 0.0}}
2025-04-19 00:03:19,350 | DEBUG | openai._base_client | _base_client.py:957 | Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-04-19 00:03:19,350 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_headers.started request=<Request [b'POST']>
2025-04-19 00:03:19,351 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_headers.complete
2025-04-19 00:03:19,351 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_body.started request=<Request [b'POST']>
2025-04-19 00:03:19,351 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_body.complete
2025-04-19 00:03:19,351 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_headers.started request=<Request [b'POST']>
2025-04-19 00:03:20,327 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 18 Apr 2025 17:03:20 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-azcwg4thexa6yyypiursgjjt'), (b'openai-processing-ms', b'680'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'4000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'3999757'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'3ms'), (b'x-request-id', b'req_1a160b8912ea1060077f3db0fae7acc3'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9325bc222e9404ec-HKG'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-04-19 00:03:20,328 | [32mINFO[0m | httpx | _client.py:1025 | HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-04-19 00:03:20,329 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_body.started request=<Request [b'POST']>
2025-04-19 00:03:20,331 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_body.complete
2025-04-19 00:03:20,332 | DEBUG | httpcore.http11 | _trace.py:47 | response_closed.started
2025-04-19 00:03:20,332 | DEBUG | httpcore.http11 | _trace.py:47 | response_closed.complete
2025-04-19 00:03:20,333 | DEBUG | openai._base_client | _base_client.py:996 | HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Fri, 18 Apr 2025 17:03:20 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'user-azcwg4thexa6yyypiursgjjt', 'openai-processing-ms': '680', 'openai-version': '2020-10-01', 'x-ratelimit-limit-requests': '5000', 'x-ratelimit-limit-tokens': '4000000', 'x-ratelimit-remaining-requests': '4999', 'x-ratelimit-remaining-tokens': '3999757', 'x-ratelimit-reset-requests': '12ms', 'x-ratelimit-reset-tokens': '3ms', 'x-request-id': 'req_1a160b8912ea1060077f3db0fae7acc3', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '9325bc222e9404ec-HKG', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-04-19 00:03:20,333 | DEBUG | openai._base_client | _base_client.py:1004 | request_id: req_1a160b8912ea1060077f3db0fae7acc3
2025-04-19 00:03:20,336 | DEBUG | openai._base_client | _base_client.py:454 | Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'You are a professional developer with experience in writing SPARQL for ontology file. Your task is to validate the provided plan to transform natural provided user query to SPARQL is valid. Please follow the instruction below:\n             - Check each step is correct. If not correct, provide how to improve in short for each step\n             - If the step level is complex, if all property is describe in previous steps, it is correct. \n             - Check if the plan can answer the user query.\n             - If plan is valid or can be accepted, the output is {"is_valid": true, "feedback": []}\n             The output format must be following this format:\n             {"is_valid": true or false, "feedback": [\n                {"step": "step query", "feedback": "feedback"}\n             ]}', 'role': 'system'}, {'content': "**User query:**What is the identity of Quang Trung in the knowledge graph?, **Plan**:[{'step': 'What is the identity of Quang Trung', 'sparql_type': 'SELECT', 'level': 'simple'}]", 'role': 'user'}], 'model': 'gpt-4o-mini', 'stream': False, 'temperature': 0.0}}
2025-04-19 00:03:20,336 | DEBUG | openai._base_client | _base_client.py:957 | Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-04-19 00:03:20,337 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_headers.started request=<Request [b'POST']>
2025-04-19 00:03:20,337 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_headers.complete
2025-04-19 00:03:20,337 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_body.started request=<Request [b'POST']>
2025-04-19 00:03:20,338 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_body.complete
2025-04-19 00:03:20,338 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_headers.started request=<Request [b'POST']>
2025-04-19 00:03:22,235 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 18 Apr 2025 17:03:22 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-azcwg4thexa6yyypiursgjjt'), (b'openai-processing-ms', b'1558'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'4000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'3999752'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'3ms'), (b'x-request-id', b'req_fa7a0be9f3392bfcfab096a9595bc6d1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9325bc285dc60ed4-HKG'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-04-19 00:03:22,235 | [32mINFO[0m | httpx | _client.py:1025 | HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-04-19 00:03:22,236 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_body.started request=<Request [b'POST']>
2025-04-19 00:03:22,239 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_body.complete
2025-04-19 00:03:22,239 | DEBUG | httpcore.http11 | _trace.py:47 | response_closed.started
2025-04-19 00:03:22,239 | DEBUG | httpcore.http11 | _trace.py:47 | response_closed.complete
2025-04-19 00:03:22,239 | DEBUG | openai._base_client | _base_client.py:996 | HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Fri, 18 Apr 2025 17:03:22 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'user-azcwg4thexa6yyypiursgjjt', 'openai-processing-ms': '1558', 'openai-version': '2020-10-01', 'x-ratelimit-limit-requests': '5000', 'x-ratelimit-limit-tokens': '4000000', 'x-ratelimit-remaining-requests': '4999', 'x-ratelimit-remaining-tokens': '3999752', 'x-ratelimit-reset-requests': '12ms', 'x-ratelimit-reset-tokens': '3ms', 'x-request-id': 'req_fa7a0be9f3392bfcfab096a9595bc6d1', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '9325bc285dc60ed4-HKG', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-04-19 00:03:22,240 | DEBUG | openai._base_client | _base_client.py:1004 | request_id: req_fa7a0be9f3392bfcfab096a9595bc6d1
2025-04-19 00:07:34,066 | DEBUG | httpcore.connection | _trace.py:47 | connect_tcp.started host='localhost' port=6333 local_address=None timeout=5.0 socket_options=None
2025-04-19 00:07:34,075 | DEBUG | httpcore.connection | _trace.py:47 | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x17fc76640>
2025-04-19 00:07:34,075 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_headers.started request=<Request [b'POST']>
2025-04-19 00:07:34,080 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_headers.complete
2025-04-19 00:07:34,080 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_body.started request=<Request [b'POST']>
2025-04-19 00:07:34,080 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_body.complete
2025-04-19 00:07:34,080 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_headers.started request=<Request [b'POST']>
2025-04-19 00:07:34,191 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'transfer-encoding', b'chunked'), (b'vary', b'accept-encoding, Origin, Access-Control-Request-Method, Access-Control-Request-Headers'), (b'content-encoding', b'zstd'), (b'content-type', b'application/json'), (b'date', b'Fri, 18 Apr 2025 17:07:34 GMT')])
2025-04-19 00:07:34,192 | [32mINFO[0m | httpx | _client.py:1025 | HTTP Request: POST http://localhost:6333/collections/refinement_examples/points/query "HTTP/1.1 200 OK"
2025-04-19 00:07:34,196 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_body.started request=<Request [b'POST']>
2025-04-19 00:07:34,196 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_body.complete
2025-04-19 00:07:34,196 | DEBUG | httpcore.http11 | _trace.py:47 | response_closed.started
2025-04-19 00:07:34,196 | DEBUG | httpcore.http11 | _trace.py:47 | response_closed.complete
2025-04-19 00:07:34,197 | DEBUG | httpcore.connection | _trace.py:47 | close.started
2025-04-19 00:07:34,197 | DEBUG | httpcore.connection | _trace.py:47 | close.complete
2025-04-19 00:07:34,832 | DEBUG | openai._base_client | _base_client.py:454 | Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'You are a professional developer with experience in writing SPARQL for ontology file. Your task is to create a plan to transform the provided natural query to SPARQL. Please follow the detailed instruction below:\n        - If a query need to compute or find out the interval of time, divide query into simple natural queries and merge queries in last step. Else please not change the query.\n        - Add level for each step: **simple**: can generate query immediately and **complex**: must use previous queries.\n        - Add type of SPARQL query for each step: SELECT, ASK, DESCRIBE and CONSTRUCT.\n        - Do not create SPARQL query.\n        - If can not create plan to transform to SPARQL, the output is []\n        The output format must be following this format: \n        [{"step": "step query", "sparql_type": "SELECT or ASK or DESCRIBE or CONSTRUCT", "level": "simple or complex"}, ...]', 'role': 'system'}, {'content': 'What is the identity of Quang Trung in the knowledge graph?', 'role': 'user'}], 'model': 'gpt-4o-mini', 'stream': False, 'temperature': 0.0}}
2025-04-19 00:07:34,832 | DEBUG | openai._base_client | _base_client.py:957 | Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-04-19 00:07:34,833 | DEBUG | httpcore.connection | _trace.py:47 | close.started
2025-04-19 00:07:34,834 | DEBUG | httpcore.connection | _trace.py:47 | close.complete
2025-04-19 00:07:34,834 | DEBUG | httpcore.connection | _trace.py:47 | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None
2025-04-19 00:07:34,875 | DEBUG | httpcore.connection | _trace.py:47 | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x17fc78400>
2025-04-19 00:07:34,875 | DEBUG | httpcore.connection | _trace.py:47 | start_tls.started ssl_context=<ssl.SSLContext object at 0x30d5ffc40> server_hostname='api.openai.com' timeout=None
2025-04-19 00:07:34,914 | DEBUG | httpcore.connection | _trace.py:47 | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x17fc76e80>
2025-04-19 00:07:34,914 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_headers.started request=<Request [b'POST']>
2025-04-19 00:07:34,914 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_headers.complete
2025-04-19 00:07:34,914 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_body.started request=<Request [b'POST']>
2025-04-19 00:07:34,914 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_body.complete
2025-04-19 00:07:34,914 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_headers.started request=<Request [b'POST']>
2025-04-19 00:07:35,863 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 18 Apr 2025 17:07:35 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-azcwg4thexa6yyypiursgjjt'), (b'openai-processing-ms', b'640'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'4000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'3999757'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'3ms'), (b'x-request-id', b'req_fd00291e3d50f7e169576bf673aae5df'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9325c25f7d3a04b6-HKG'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-04-19 00:07:35,864 | [32mINFO[0m | httpx | _client.py:1025 | HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-04-19 00:07:35,866 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_body.started request=<Request [b'POST']>
2025-04-19 00:07:35,866 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_body.complete
2025-04-19 00:07:35,866 | DEBUG | httpcore.http11 | _trace.py:47 | response_closed.started
2025-04-19 00:07:35,866 | DEBUG | httpcore.http11 | _trace.py:47 | response_closed.complete
2025-04-19 00:07:35,866 | DEBUG | openai._base_client | _base_client.py:996 | HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Fri, 18 Apr 2025 17:07:35 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'user-azcwg4thexa6yyypiursgjjt', 'openai-processing-ms': '640', 'openai-version': '2020-10-01', 'x-ratelimit-limit-requests': '5000', 'x-ratelimit-limit-tokens': '4000000', 'x-ratelimit-remaining-requests': '4999', 'x-ratelimit-remaining-tokens': '3999757', 'x-ratelimit-reset-requests': '12ms', 'x-ratelimit-reset-tokens': '3ms', 'x-request-id': 'req_fd00291e3d50f7e169576bf673aae5df', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '9325c25f7d3a04b6-HKG', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-04-19 00:07:35,867 | DEBUG | openai._base_client | _base_client.py:1004 | request_id: req_fd00291e3d50f7e169576bf673aae5df
2025-04-19 00:07:35,871 | DEBUG | openai._base_client | _base_client.py:454 | Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'You are a professional developer with experience in writing SPARQL for ontology file. Your task is to validate the provided plan to transform natural provided user query to SPARQL is valid. Please follow the instruction below:\n             - Check each step is correct. If not correct, provide how to improve in short for each step\n             - If the step level is complex, if all property is describe in previous steps, it is correct. \n             - Check if the plan can answer the user query.\n             - If plan is valid or can be accepted, the output is {"is_valid": true, "feedback": []}\n             The output format must be following this format:\n             {"is_valid": true or false, "feedback": [\n                {"step": "step query", "feedback": "feedback"}\n             ]}', 'role': 'system'}, {'content': "**User query:**What is the identity of Quang Trung in the knowledge graph?, **Plan**:[{'step': 'What is the identity of Quang Trung', 'sparql_type': 'SELECT', 'level': 'simple'}]", 'role': 'user'}], 'model': 'gpt-4o-mini', 'stream': False, 'temperature': 0.0}}
2025-04-19 00:07:35,872 | DEBUG | openai._base_client | _base_client.py:957 | Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-04-19 00:07:35,872 | DEBUG | httpcore.connection | _trace.py:47 | close.started
2025-04-19 00:07:35,872 | DEBUG | httpcore.connection | _trace.py:47 | close.complete
2025-04-19 00:07:35,873 | DEBUG | httpcore.connection | _trace.py:47 | connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None
2025-04-19 00:07:35,913 | DEBUG | httpcore.connection | _trace.py:47 | connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x17fc78250>
2025-04-19 00:07:35,913 | DEBUG | httpcore.connection | _trace.py:47 | start_tls.started ssl_context=<ssl.SSLContext object at 0x30d5ffd40> server_hostname='api.openai.com' timeout=None
2025-04-19 00:07:35,955 | DEBUG | httpcore.connection | _trace.py:47 | start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x17fc78220>
2025-04-19 00:07:35,955 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_headers.started request=<Request [b'POST']>
2025-04-19 00:07:35,955 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_headers.complete
2025-04-19 00:07:35,955 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_body.started request=<Request [b'POST']>
2025-04-19 00:07:35,955 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_body.complete
2025-04-19 00:07:35,955 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_headers.started request=<Request [b'POST']>
2025-04-19 00:07:38,720 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 18 Apr 2025 17:07:38 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-azcwg4thexa6yyypiursgjjt'), (b'openai-processing-ms', b'2029'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'4000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'3999752'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'3ms'), (b'x-request-id', b'req_e7120dce97e658374c01eefde398bb25'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9325c2660c5220f0-HKG'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-04-19 00:07:38,721 | [32mINFO[0m | httpx | _client.py:1025 | HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-04-19 00:07:38,722 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_body.started request=<Request [b'POST']>
2025-04-19 00:07:38,730 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_body.complete
2025-04-19 00:07:38,731 | DEBUG | httpcore.http11 | _trace.py:47 | response_closed.started
2025-04-19 00:07:38,731 | DEBUG | httpcore.http11 | _trace.py:47 | response_closed.complete
2025-04-19 00:07:38,731 | DEBUG | openai._base_client | _base_client.py:996 | HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Fri, 18 Apr 2025 17:07:38 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'user-azcwg4thexa6yyypiursgjjt', 'openai-processing-ms': '2029', 'openai-version': '2020-10-01', 'x-ratelimit-limit-requests': '5000', 'x-ratelimit-limit-tokens': '4000000', 'x-ratelimit-remaining-requests': '4999', 'x-ratelimit-remaining-tokens': '3999752', 'x-ratelimit-reset-requests': '12ms', 'x-ratelimit-reset-tokens': '3ms', 'x-request-id': 'req_e7120dce97e658374c01eefde398bb25', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '9325c2660c5220f0-HKG', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-04-19 00:07:38,732 | DEBUG | openai._base_client | _base_client.py:1004 | request_id: req_e7120dce97e658374c01eefde398bb25
2025-04-19 00:07:38,736 | DEBUG | openai._base_client | _base_client.py:454 | Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'You are a professional developer with experience in writing SPARQL for ontology file. Your task is to create a plan to transform the provided natural query to SPARQL. Please follow the detailed instruction below:\n        - If a query need to compute or find out the interval of time, divide query into simple natural queries and merge queries in last step. Else please not change the query.\n        - Add level for each step: **simple**: can generate query immediately and **complex**: must use previous queries.\n        - Add type of SPARQL query for each step: SELECT, ASK, DESCRIBE and CONSTRUCT.\n        - Do not create SPARQL query.\n        - If can not create plan to transform to SPARQL, the output is []\n        The output format must be following this format: \n        [{"step": "step query", "sparql_type": "SELECT or ASK or DESCRIBE or CONSTRUCT", "level": "simple or complex"}, ...]', 'role': 'system'}, {'content': 'What is the identity of Quang Trung in the knowledge graph?', 'role': 'user'}], 'model': 'gpt-4o-mini', 'stream': False, 'temperature': 0.0}}
2025-04-19 00:07:38,737 | DEBUG | openai._base_client | _base_client.py:957 | Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-04-19 00:07:38,738 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_headers.started request=<Request [b'POST']>
2025-04-19 00:07:38,739 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_headers.complete
2025-04-19 00:07:38,739 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_body.started request=<Request [b'POST']>
2025-04-19 00:07:38,740 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_body.complete
2025-04-19 00:07:38,740 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_headers.started request=<Request [b'POST']>
2025-04-19 00:07:39,700 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 18 Apr 2025 17:07:39 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-azcwg4thexa6yyypiursgjjt'), (b'openai-processing-ms', b'669'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'4000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'3999757'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'3ms'), (b'x-request-id', b'req_1a00e7be699bb7b9bed1e82aacd9b8a5'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9325c2775a5e04b6-HKG'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-04-19 00:07:39,700 | [32mINFO[0m | httpx | _client.py:1025 | HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-04-19 00:07:39,701 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_body.started request=<Request [b'POST']>
2025-04-19 00:07:39,704 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_body.complete
2025-04-19 00:07:39,704 | DEBUG | httpcore.http11 | _trace.py:47 | response_closed.started
2025-04-19 00:07:39,704 | DEBUG | httpcore.http11 | _trace.py:47 | response_closed.complete
2025-04-19 00:07:39,704 | DEBUG | openai._base_client | _base_client.py:996 | HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Fri, 18 Apr 2025 17:07:39 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'user-azcwg4thexa6yyypiursgjjt', 'openai-processing-ms': '669', 'openai-version': '2020-10-01', 'x-ratelimit-limit-requests': '5000', 'x-ratelimit-limit-tokens': '4000000', 'x-ratelimit-remaining-requests': '4999', 'x-ratelimit-remaining-tokens': '3999757', 'x-ratelimit-reset-requests': '12ms', 'x-ratelimit-reset-tokens': '3ms', 'x-request-id': 'req_1a00e7be699bb7b9bed1e82aacd9b8a5', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '9325c2775a5e04b6-HKG', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-04-19 00:07:39,705 | DEBUG | openai._base_client | _base_client.py:1004 | request_id: req_1a00e7be699bb7b9bed1e82aacd9b8a5
2025-04-19 00:07:39,706 | DEBUG | openai._base_client | _base_client.py:454 | Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'You are a professional developer with experience in writing SPARQL for ontology file. Your task is to validate the provided plan to transform natural provided user query to SPARQL is valid. Please follow the instruction below:\n             - Check each step is correct. If not correct, provide how to improve in short for each step\n             - If the step level is complex, if all property is describe in previous steps, it is correct. \n             - Check if the plan can answer the user query.\n             - If plan is valid or can be accepted, the output is {"is_valid": true, "feedback": []}\n             The output format must be following this format:\n             {"is_valid": true or false, "feedback": [\n                {"step": "step query", "feedback": "feedback"}\n             ]}', 'role': 'system'}, {'content': "**User query:**What is the identity of Quang Trung in the knowledge graph?, **Plan**:[{'step': 'What is the identity of Quang Trung', 'sparql_type': 'SELECT', 'level': 'simple'}]", 'role': 'user'}], 'model': 'gpt-4o-mini', 'stream': False, 'temperature': 0.0}}
2025-04-19 00:07:39,707 | DEBUG | openai._base_client | _base_client.py:957 | Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-04-19 00:07:39,707 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_headers.started request=<Request [b'POST']>
2025-04-19 00:07:39,707 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_headers.complete
2025-04-19 00:07:39,707 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_body.started request=<Request [b'POST']>
2025-04-19 00:07:39,707 | DEBUG | httpcore.http11 | _trace.py:47 | send_request_body.complete
2025-04-19 00:07:39,707 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_headers.started request=<Request [b'POST']>
2025-04-19 00:07:41,682 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 18 Apr 2025 17:07:41 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-azcwg4thexa6yyypiursgjjt'), (b'openai-processing-ms', b'1656'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'4000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'3999752'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'3ms'), (b'x-request-id', b'req_3ff1057769d51e36be30a7b15a6a6bf4'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9325c27d6fdb20f0-HKG'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-04-19 00:07:41,683 | [32mINFO[0m | httpx | _client.py:1025 | HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-04-19 00:07:41,683 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_body.started request=<Request [b'POST']>
2025-04-19 00:07:41,684 | DEBUG | httpcore.http11 | _trace.py:47 | receive_response_body.complete
2025-04-19 00:07:41,684 | DEBUG | httpcore.http11 | _trace.py:47 | response_closed.started
2025-04-19 00:07:41,684 | DEBUG | httpcore.http11 | _trace.py:47 | response_closed.complete
2025-04-19 00:07:41,684 | DEBUG | openai._base_client | _base_client.py:996 | HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Fri, 18 Apr 2025 17:07:41 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'user-azcwg4thexa6yyypiursgjjt', 'openai-processing-ms': '1656', 'openai-version': '2020-10-01', 'x-ratelimit-limit-requests': '5000', 'x-ratelimit-limit-tokens': '4000000', 'x-ratelimit-remaining-requests': '4999', 'x-ratelimit-remaining-tokens': '3999752', 'x-ratelimit-reset-requests': '12ms', 'x-ratelimit-reset-tokens': '3ms', 'x-request-id': 'req_3ff1057769d51e36be30a7b15a6a6bf4', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '9325c27d6fdb20f0-HKG', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-04-19 00:07:41,684 | DEBUG | openai._base_client | _base_client.py:1004 | request_id: req_3ff1057769d51e36be30a7b15a6a6bf4
